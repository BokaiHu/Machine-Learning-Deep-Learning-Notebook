# Scaling Law

- $L(x) = L(\infty) + (\frac{x_0}{x})^{\alpha}$. 此处$x$为模型计算量，通过增加模型的FLOP可以减少"可优化的loss"，即真实分布与模型分布之间的差距，使最终的loss接近"无法优化的loss"，即数据本身包含的噪音。
- 另外模型的性能与计算量呈幂律关系，可以通过一系列小模型的性能估计最终目标大模型的性能。

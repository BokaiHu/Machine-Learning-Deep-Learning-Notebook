## Ensemble

### GBDT

GBDT的原理在于用下一颗树拟合上一颗树输出与ground truth之间损失函数的负梯度。类比梯度下降过程的话，gradient descent是对模型参数$\theta$求导，不断更新模型参数来获得最小的损失函数；而GBDT中我们直接用损失函数对上一颗树的输出求导，这时得到的负梯度就代表了为了使损失函数最小需要将模型预测的输出向某个方向移动某个数值，此时我们就用下一颗树来拟合这个数值即可。

和XGBoost相比，这里相当于一阶泰勒展开。

### Adaboost

1. 获得一个error rate最小的弱分类器。
2. 计算该弱分类器的权重: $\alpha_t=0.5log\frac{1-\epsilon}{\epsilon}$
3. 更新每个样本的权重: $D_{i,t}=D_{i, t-1}*exp(-\alpha_ty_ih(x_i))$, 预测错了$y_ih(x_i)=-1$, 权重就会增大。
4. 重复2-3

### XGBoost

利用泰勒展开计算一个$obj=\sum_j(G_jw_j+0.5w_j^2(H_j+\gamma))$, 利用obj寻找最优分裂点。Gain代表了分裂后的目标函数与分裂前目标函数的差值，下降越大gain越大。

## Classifier

### SVM

定义超平面$w^Tx+b=0$, 定义所有正例为$w^Tx+b\geq1$. 优化目标是找到一对支撑向量使得$\frac{1}{||w||}$最大，等价于使得$0.5||w||^2$最小。所以优化目标变为：

$$
argmin_w\frac{1}{2}||w||^2,\,y_i(w^Tx_i+b)\geq1
$$

用拉格朗日算子进行转换后：

$$
argmin_w\frac{1}{2}||w||^2-\sum_i\alpha_i(y_i(w^Tx_i+b)-1)
$$

对$w$, $b$分别求导

$$
\frac{\partial Obj}{\partial w}=w-\sum_i\alpha_iy_ix_i=0 \\
\frac{\partial Obj}{\partial b}=\sum_i\alpha_iy_i=0
$$

带回去得到:

$$
argmax_{\alpha_i}\sum_i\alpha_i-\frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy_iy_jx_ix_j
$$

可以用优化器求数值解。


## 优缺点总结

### 逻辑回归

- 优点：
  - 简单，返回一个概率值，可解释性强。
  - 对异常点不那么敏感。
  - 可以应用正则化缓解
  - 可以使用PCA解决多重共线性假设的问题。
- 缺点：
  - 变量太多时效果不佳。
  - 需要特征与目标变量呈线性关系，并且残差为高斯分布。
  - 在数据不平衡时会偏向多数类。
  - 严重依赖特征工程。

### 决策树

- 优点
  - 可解释性强，每一步都能可视化判断依据。
  - 可以处理非线性关系以及缺失值。
  - 自动进行特征工程。
  - 可以组合成更强的分类器共同决策。
  - 非参数化，可以进行分类和回归。
- 缺点：
  - 容易过拟合训练集。
  - 表现能力有限， 面对复杂的分类边界可能无法很好处理。
  - 可能偏向多数类。

### SVM

- 优点：
  - 可以处理高维输入，在高维空间中表现也很好。
  - 可以处理复杂的非线性边界，通过调整核函数。
  - 只需要保存支持向量就可以进行推理。
- 缺点：
  - 计算速度慢。
  - 黑盒，不好解释。
  - 对异常值或者噪音不robust。
  - 需要精细调参。

### RandomForest

- 优点
  - Ensemble，不容易过拟合。
  - 能捕捉复杂的非线性关系。
  - 一定程度上可解释。
- 缺点
  - 无法拟合极值，无法拟合超出数据范围的值。

### XGBoost

- 优点
  - Ensemble，且添加了正则化。
  - 能捕捉复杂的非线性关系。
  - 可解释性，可以看叶片的权重。
  - 通过优化可并行，加快速度。
  - 性能优异。
- 缺点
  - 复杂，可能过拟合，需要精细调整参数。
- 缺点

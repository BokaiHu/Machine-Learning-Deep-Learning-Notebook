{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channel: int,\n",
    "                 latent_dim: int, \n",
    "                 hidden_dims: list = None\n",
    "                ) -> None:\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        # Encoder\n",
    "        modules = []\n",
    "        for dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channel, dim, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(dim),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "            in_channel = dim\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.mu = nn.Linear(in_channel * 4, latent_dim)\n",
    "        self.log_var = nn.Linear(in_channel * 4, latent_dim)\n",
    "\n",
    "        self.zt = nn.Linear(latent_dim, in_channel * 4)\n",
    "\n",
    "        # Decoder\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i], hidden_dims[i + 1], kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dims[-1], hidden_dims[-1], kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dims[-1], 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.log_var(x)\n",
    "        return (mu, log_var)\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.zt(z)\n",
    "        z = z.view(-1, self.hidden_dims[0], 2, 2)\n",
    "        x = self.decoder(z)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, _, _, _ = x.shape\n",
    "        mu, log_var = self.encode(x)\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.rand_like(std)\n",
    "\n",
    "        # Reparametrization\n",
    "        z = std * eps + mu\n",
    "        re_x = self.decode(z)\n",
    "        return re_x\n",
    "    \n",
    "    def loss(self, x, reg):\n",
    "        re_x = self.forward(x)\n",
    "        mu, log_var = self.encode(x)\n",
    "        mse = F.mse_loss(x, re_x)\n",
    "        kl_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = mse + reg * kl_loss\n",
    "        return loss\n",
    "\n",
    "    def inference(self, num_samples, device):\n",
    "        z = torch.rand(num_samples, self.latent_dim).to(device=device)\n",
    "\n",
    "        re_x = self.decode(z)\n",
    "        return re_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, dict_size, dimension, beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.K = dict_size\n",
    "        self.D = dimension\n",
    "        self.beta = beta\n",
    "        self.embedding = nn.Embedding(self.K, self.D)\n",
    "        self.embedding.weight.data.uniform_(-1 / self.K, 1 / self.K)\n",
    "\n",
    "    def forward(self, code):\n",
    "        code = code.permute(0, 2, 3, 1).contiguous() # [B, H, W, D]\n",
    "        B, H, W, D = code.shape\n",
    "        flat_code = code.view(-1, self.D) # [BHW, D]\n",
    "\n",
    "        l2 = torch.sum(flat_code ** 2, dim=1, keepdim=True) + torch.sum(self.embedding.weight ** 2, dim=1) - 2 * torch.matmul(flat_code, self.embedding.weight.t()) # [BHW, K]\n",
    "        min_indices = torch.argmin(l2, dim=1).unsqueeze(1) # [BHW, 1]\n",
    "\n",
    "        onehot_encoding = torch.zeros((B * H * W, self.K), device=code.device)\n",
    "        onehot_encoding.scatter_(1, min_indices, 1) # [BHW, K]\n",
    "\n",
    "        cb_encoding = torch.matmul(onehot_encoding, self.embedding.weight) # [BHW, D]\n",
    "        cb_encoding = cb_encoding.view(B, H, W, D)\n",
    "\n",
    "        zq_loss = F.mse_loss(cb_encoding, code.detach())\n",
    "        z_loss = F.mse_loss(cb_encoding.detach(), code)\n",
    "\n",
    "        loss = zq_loss * self.beta + z_loss\n",
    "        q_code = cb_encoding + (code - cb_encoding).detach()\n",
    "        q_code = q_code.permute(0, 3, 1, 2).contiguous()\n",
    "        return q_code, loss\n",
    "    \n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.residule = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channel, self.out_channel, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.out_channel, self.out_channel, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.residule(x)\n",
    "    \n",
    "class VQ_VAE(nn.Module):\n",
    "    def __init__(self, in_channel: int,\n",
    "                 embedding_dim: int, \n",
    "                 dict_size: int,\n",
    "                 beta: float,\n",
    "                 hidden_dims: list = None,\n",
    "                ) -> None:\n",
    "        super(VQ_VAE, self).__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [128, 256]\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        # Encoder\n",
    "        modules = []\n",
    "        for dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channel, dim, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "            in_channel = dim\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channel, in_channel, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        )\n",
    "        for i in range(0, 6):\n",
    "            modules.append(ResidualLayer(in_channel, in_channel))\n",
    "        modules.append(nn.ReLU())\n",
    "\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channel, in_channel, kernel_size=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.vq = VectorQuantizer(dict_size, embedding_dim, beta)\n",
    "\n",
    "        # Decoder\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i], hidden_dims[i + 1], kernel_size=4, stride=2, padding=1, output_padding=0),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dims[-1], hidden_dims[-1], kernel_size=4, stride=2, padding=1, output_padding=0),\n",
    "            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dims[-1], 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.decoder(z)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        q_code, cb_loss = self.vq(x)\n",
    "        re_x = self.decode(q_code)\n",
    "        return re_x, cb_loss\n",
    "    \n",
    "    def loss(self, x):\n",
    "        re_x, cb_loss = self.forward(x)\n",
    "        mse = F.mse_loss(x, re_x)\n",
    "\n",
    "        loss = mse + cb_loss\n",
    "        return loss\n",
    "\n",
    "    def inference(self, num_samples, device):\n",
    "        z = torch.rand(num_samples, self.latent_dim).to(device=device)\n",
    "\n",
    "        re_x = self.decode(z)\n",
    "        return re_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "    def __init__(self, input_dim, n_heads=8, qkv_bias=False, qk_scale=None, attn_dropout=0., proj_dropout=0.):\n",
    "        super().__init__(self, self_attention)\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = input_dim // n_heads\n",
    "        self.qkv = nn.Linear(input_dim, input * 3, bias=qkv_bias) # Calculate q, k, v using a single FC.\n",
    "        self.attn_drop = nn.Dropout(attn_dropout)\n",
    "        self.proj = nn.Linear(input_dim, input_dim)\n",
    "        self.proj_dropout = nn.Dropout(proj_dropout)\n",
    "        self.qk_scale = qk_scale or self.head_dim ** (-0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, L = x.shape # (batch_size, number of sequence, sequence length)\n",
    "    \n",
    "        ## (B, N, 3, n_heads, head_dim) -> (3, B, n_heads, N, head_dim)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # (B, n_heads, N, head_dim)\n",
    "    \n",
    "        ## (B, n_heads, N, head_dim) * (B, n_heads, head_dim, N) -> (B, n_heads, N, N)\n",
    "        attn_score = (q @ k.transpose(-2, -1)).softmax(axis=-1) * self.qk_scale\n",
    "    \n",
    "        ## (B, n_heads, N, N) * (B, n_heads, N, head_dim) -> (B, N, n_heads, head_dim) -> (B, N, L)\n",
    "        weighted_sum = (attn_score @ v).transpose(1, 2).reshape(B, N, L)\n",
    "        weighted_sum = self.attn_dropout(weighted_sum)\n",
    "        proj = self.proj(weighted_sum)\n",
    "        proj = self.proj_dropout(proj)\n",
    "    \n",
    "        return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196, 196])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads # 384/6=64 每个head中的token的维度\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5 # 计算Q和K的相似度时分母用到的数值=1/sqrt(64)=0.125\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) # 一次FC同时得到Q，K以及V三个矩阵\n",
    "        self.attn_drop = nn.Dropout(attn_drop) # dropout:0-0.2, 12个等差数列\n",
    "        self.proj = nn.Linear(dim, dim) # 多个head的输出进行concat后，再做一次矩阵变换得到multi-head attention的结果\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape  # [batch_size, num_patches+1(class token), total_embed_dim]\n",
    "\n",
    "        # qkv(x): [batch_size, num_patches+1, 3*total_embed_dim] = [batchsize, 197, 3*384]\n",
    "        # reshape() -> permute: [batchsize, num_patches+1, 3, 6, 384/6] -> [3, batchsize, 6, num_patches+1, 64]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # 获取q，k以及v矩阵，[batchsize, 6, 197, 64]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "        # 将key矩阵的最后两个维度进行转置，高维矩阵乘法转换成两个维度的矩阵乘法 [batchsize, 6, 197, 64] * [batchsize, 6, 64, 197]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale # [batchsize, 6, 197, 197]\n",
    "        attn = attn.softmax(dim=-1) # 在最后一个维度上进行softmax也就是针对每一行进行softmax\n",
    "        attn = self.attn_drop(attn)\n",
    "        # attention * v：[batchsize, 6, 197, 64] -> [batchsize, 197, 6, 64] -> [batchsize, 197, 384]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)      # 进行一个线性变换得到multi-head attention的输出 [batch, 197, 384]\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "a = torch.rand((4, 196, 384))\n",
    "t = Attention(384, 4, False)\n",
    "t1 = nn.Linear(384, 196)\n",
    "t1(a).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
